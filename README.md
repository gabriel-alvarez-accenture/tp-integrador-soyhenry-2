üìä Relevancia de los Datos
Este proyecto contempla dos casos principales de ingesti√≥n de datos, cada uno con su propia fuente y tratamiento:

üóÇÔ∏è Caso 1: Archivo CSV
El archivo .csv ya se encuentra ubicado en el repositorio, por lo tanto, la fuente de datos es el propio archivo. Este atraviesa todo el proceso ELT (Extract, Load, Transform) para generar informaci√≥n relevante para el negocio.
Durante este proceso se generan tablas de dimensiones y una tabla de hechos, lo que permite estructurar los datos de forma clara y facilitar su an√°lisis.

üåê Caso 2: Consumo de API
En este caso, los datos se obtienen desde una API externa. La consulta es totalmente parametrizable mediante el archivo de entorno .env, lo que evita el hardcodeo y permite una configuraci√≥n flexible.
Los datos extra√≠dos se procesan para obtener una tabla √∫nica y refinada, lista para ser utilizada en an√°lisis posteriores.



üèóÔ∏è Arquitectura de los Datos
Este proyecto se apoya en un stack moderno de herramientas que permiten construir una arquitectura robusta, escalable y trazable para procesos ELT.

üîÑ Airflow
Airflow fue elegido por su capacidad de escalar en entornos distribuidos, su integraci√≥n nativa con m√∫ltiples herramientas del ecosistema de datos, y su eficiencia para orquestar procesos complejos de forma robusta, trazable y modular.
Es ideal para coordinar flujos ELT, permitiendo definir dependencias, programar tareas y monitorear ejecuciones con facilidad.

üß± DBT (Data Build Tool)
DBT permite transformar datos directamente sobre el motor SQL de forma modular, eficiente y controlada.
Su integraci√≥n con Airflow y Git facilita la escalabilidad, el versionado y la calidad del c√≥digo, lo que lo convierte en una herramienta clave en entornos anal√≠ticos modernos.

üóÑÔ∏è MinIO
MinIO proporciona un almacenamiento de objetos altamente escalable y compatible con S3, ideal para manejar datos crudos de forma eficiente y econ√≥mica.
Ofrece buena performance, f√°cil integraci√≥n con el ecosistema de datos, y a√±ade una capa de seguridad mediante el cifrado de objetos.

üêò PostgreSQL
PostgreSQL se utiliza como base de datos relacional por su rendimiento s√≥lido, bajo costo y amplia compatibilidad con herramientas del stack.
Es especialmente adecuado como motor SQL para transformaciones con DBT en entornos de tama√±o medio, y tambi√©n sirve como repositorio para las capas STAGING, CORE y CONSUMO.



üß¨ Capas de la Arquitectura de Datos
La arquitectura de datos se organiza en capas bien definidas que permiten estructurar el pipeline ELT de forma clara, escalable y mantenible:

üü§ RAW (MinIO)
Almacena los datos extra√≠dos directamente desde las fuentes, sin ning√∫n tipo de procesamiento.
Representa una copia fiel del origen y es el punto de partida del pipeline.
Ideal para conservar la trazabilidad y auditar los datos originales.
üü° STAGING (PostgreSQL)
Recibe los datos desde la capa RAW para realizar tareas de limpieza, normalizaci√≥n y preparaci√≥n.
Es una capa transitoria que facilita las transformaciones posteriores.
Permite desacoplar la extracci√≥n de la transformaci√≥n, mejorando la modularidad del pipeline.
üü¢ CORE (PostgreSQL + dbt)
Contiene los datos transformados en modelos de negocio, como tablas de hechos y dimensiones.
Es la √∫ltima capa del proceso ELT, donde los datos est√°n listos para an√°lisis y toma de decisiones.
Las transformaciones se realizan con dbt, asegurando calidad, versionado y trazabilidad.
üîµ CONSUMO (PostgreSQL)
Aunque el proceso ELT finaliza en la capa CORE, PostgreSQL tambi√©n alberga una capa de consumo.
Aqu√≠ los usuarios finales pueden crear vistas, m√©tricas y dashboards personalizados seg√∫n sus necesidades.
Facilita el acceso a la informaci√≥n de forma flexible y orientada al negocio.



üê≥ C√≥mo Ejecutar el Pipeline ELT con Docker
Este proyecto utiliza un Dockerfile personalizado para construir un entorno de Airflow con todas las dependencias necesarias. A continuaci√≥n, se detallan los pasos para ejecutar el pipeline completo:

1Ô∏è‚É£ Construcci√≥n de la Imagen Personalizada
Constru√≠ la imagen de Docker a partir del Dockerfile, instalando las dependencias definidas en requirements.txt:
    docker build -t mi-airflow:latest .

2Ô∏è‚É£ Levantamiento del Entorno con Docker Compose
Inici√° el entorno de Airflow en segundo plano, construyendo los servicios si es necesario:
    docker-compose up -d --build

3Ô∏è‚É£ Ejecuci√≥n del Pipeline
Una vez iniciado el entorno, Airflow ejecutar√° el DAG que orquesta el proceso ELT completo:
    Extracci√≥n de datos desde las fuentes (CSV o API).
    Almacenamiento en MinIO (capa RAW).
    Carga en PostgreSQL (capa STAGING).
    Transformaci√≥n con dbt (capa CORE).

4Ô∏è‚É£ Finalizaci√≥n del Entorno
Para detener y eliminar los contenedores:
    docker-compose down
